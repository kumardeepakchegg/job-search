# How Admin Scrapes Data: Complete Walkthrough

## ğŸ“ Admin Dashboard Entry Point

### Step 1: Admin Logs In
```
URL: http://localhost:8080/login
Email: admin@jobintel.local
Password: AdminPass!23
```

**What happens:**
- Backend authenticates credentials
- Creates JWT token with `role: "admin"`
- Stores token in localStorage
- Redirects to `/admin` dashboard

---

## ğŸ¯ Navigate to Crawlers Page

### Step 2: Open Admin Sidebar
```
URL: http://localhost:8080/admin
```

**You see:**
- Left sidebar with 11 menu items
- Top navbar with admin controls
- Main content area

### Step 3: Click "Crawlers" in Sidebar
```
Click: Globe icon + "Crawlers" text
OR
Navigate directly to: http://localhost:8080/admin/crawlers
```

**Page loads with:**
- âœ… "Web Crawlers & Scraping" title
- âœ… "Trigger Scraping Job" card
- âœ… 11 job bucket checkboxes
- âœ… "Start Scraping" button
- âœ… "Scraping Logs" table below

---

## ğŸš€ Trigger Scraping Job

### Step 4: Select Job Buckets

**Option A: Select Individual Buckets**
```
â˜‘ fresher
â˜‘ software  
â˜‘ data
â˜ cloud
â˜ mobile
...
```

**Option B: Select All**
```
Click "Select All" button
â†’ All 11 buckets checked
```

**Available Buckets:**
1. **fresher** - Entry-level jobs (0-1 years)
2. **batch** - Campus/batch hiring
3. **software** - Developer/engineer roles
4. **data** - Data scientist/engineer roles
5. **cloud** - DevOps/cloud engineer roles
6. **mobile** - Mobile app developer roles
7. **qa** - QA/testing engineer roles
8. **non-tech** - Non-technical roles
9. **experience** - Senior/experienced roles
10. **employment** - Full-time/part-time/contract
11. **work-mode** - Remote/hybrid/onsite

### Step 5: Review API Usage Info
```
Info Box Shows:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Monthly API Limit: 200 calls/month
â”‚ Rate Limited: 1 request/sec
â”‚ Selected Buckets: 3 / 11        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â–¶ï¸ Start Scraping

### Step 6: Click "Start Scraping" Button

```typescript
// Frontend Action
POST /api/admin/scrape/run
{
  "buckets": ["fresher", "software", "data"],
  "triggeredBy": "admin"
}
```

**Frontend shows:**
- Button loading state
- Alert: "Scraping started! Session ID: session_1234567890"

---

## ğŸ”„ Backend Processing

### Step 7: API Budget Check

```typescript
// Backend checks monthly limit
if (apiUsage.totalCallsUsed >= 200) {
  throw Error("Monthly API limit reached");
}
```

**Current Usage Example:**
- Used: 45 calls
- Limit: 200 calls
- Remaining: 155 calls âœ…

### Step 8: For Each Selected Bucket

```typescript
// Rate Limited: 1 request per second
for (const bucket of ["fresher", "software", "data"]) {
  
  // Call OpenWeb Ninja API
  const response = await openWebNinjaClient.search({
    q: BUCKET_KEYWORD[bucket], // e.g., "fresher developer"
    country: "in",
    limit: 100,
    ...
  });

  // Result: ~50-100 jobs per bucket
  const rawJobs = response.data; // Array of raw job objects
  
  // For each job:
  for (const rawJob of rawJobs) {
    
    // 1. NORMALIZE: Extract 30+ fields
    const normalized = jobNormalizationService.normalize(rawJob);
    // Result example:
    // {
    //   title: "Senior React Developer",
    //   careerLevel: "senior",
    //   domain: "software",
    //   techStack: ["React", "Node.js", "MongoDB"],
    //   workMode: "remote",
    //   externalJobId: "openweb_12345", // UNIQUE KEY
    //   ...
    // }

    // 2. DEDUPLICATE: Check if already in DB
    const existing = await Job.findOne({
      externalJobId: normalized.externalJobId
    });

    if (!existing) {
      // NEW JOB: Insert
      await Job.create(normalized);
      newJobsAdded++;
    } else {
      // DUPLICATE: Update with latest info
      await Job.updateOne(
        { externalJobId: normalized.externalJobId },
        normalized
      );
      jobsUpdated++;
    }
  }

  // Wait 1 second before next bucket (rate limit)
  await delay(1000);
}
```

---

## ğŸ“Š Example: Scraping "Fresher" Bucket

```javascript
// Raw API Response from OpenWeb Ninja
[
  {
    "title": "Junior Java Developer",
    "company": "TechCorp",
    "location": "Bangalore",
    "description": "Looking for fresher Java developers...",
    "job_is_remote": false,
    "salary": "â‚¹4-6 LPA",
    "apply_url": "https://...",
    ...
  },
  {
    "title": "Fresher Python Developer",
    "company": "CloudTech",
    "location": "Remote",
    ...
  },
  ... (78+ more jobs)
]

â†“ NORMALIZATION SERVICE â†“

// Normalized for Database
[
  {
    _id: ObjectId(...),
    title: "Junior Java Developer",
    careerLevel: "fresher",        // Detected from "junior"
    domain: "software",             // Detected from job title
    techStack: ["Java", "Spring"],  // Extracted from description
    workMode: "onsite",             // From job_is_remote: false
    externalJobId: "openweb_123",   // Unique key for deduplication
    source: "OpenWeb Ninja",
    bucket: "fresher",
    fetchedAt: 2025-01-19T10:30Z,
    expiryDate: 2025-02-18T10:30Z,  // 30 days later
    isActive: true,
    ...
  },
  ...
]

â†“ SAVED TO MONGODB â†“

Database: jobs collection
- New jobs: 71 added
- Duplicate updates: 7 updated
- Total processed: 78
```

---

## ğŸ“ Create Scraping Log Entry

```typescript
// After completing all buckets, create log
const log = await ScrapingLog.create({
  sessionId: "session_1234567890",
  status: "completed",
  bucketsRequested: ["fresher", "software", "data"],
  bucketsCompleted: ["fresher", "software", "data"],
  bucketsFailed: [],
  totalApiCalls: 3,
  totalJobsFound: 234,      // 78 + 82 + 74
  newJobsAdded: 212,        // 71 + 74 + 67
  jobsUpdated: 22,          // 7 + 8 + 7
  startedAt: "2025-01-19T10:30:00Z",
  completedAt: "2025-01-19T10:35:00Z",
  durationMs: 300000,       // 5 minutes
  triggeredBy: "admin",
  triggeredByUserId: ObjectId("admin_id"),
  bucketDetails: [
    {
      bucket: "fresher",
      apiCallsMade: 1,
      jobsFound: 78,
      newJobsAdded: 71,
      jobsUpdated: 7,
      status: "success"
    },
    {
      bucket: "software",
      apiCallsMade: 1,
      jobsFound: 82,
      newJobsAdded: 74,
      jobsUpdated: 8,
      status: "success"
    },
    {
      bucket: "data",
      apiCallsMade: 1,
      jobsFound: 74,
      newJobsAdded: 67,
      jobsUpdated: 7,
      status: "success"
    }
  ]
});

// API Usage updated
await ApiUsage.updateOne(
  { month: "2025-01" },
  { 
    $inc: { totalCallsUsed: 3 },
    callsRemaining: 197
  }
);
```

---

## ğŸ“‹ Logs Display in Frontend

### Step 9: View Scraping Logs

**Auto-refresh after 2 seconds**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCRAPING LOGS                  Refresh â†» â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚
â”‚ Session ID: session_1234567890 [COMPLETED]
â”‚ Started: Jan 19, 2025, 10:30 AM
â”‚
â”‚ API Calls: 3      Jobs Found: 234
â”‚ New Added: 212    Updated: 22
â”‚
â”‚ Completed Buckets: [fresher] [software] [data]
â”‚ Failed Buckets: (none)
â”‚
â”‚ Duration: 300.00s
â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚
â”‚ Session ID: session_0987654321 [COMPLETED]
â”‚ Started: Jan 18, 2025, 3:15 PM
â”‚ ... (previous scraping session)
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ‘¥ User Sees Matched Jobs

### Step 10: User Uploads Resume

```
URL: http://localhost:8080/dashboard
Click: "Upload Resume"
Select: resume.pdf
```

**Backend Process:**
1. Extract text from PDF/DOCX
2. Parse skills using regex (100+ technology database)
3. Extract work history, education, experience
4. Save to `parsed_resumes` collection
5. **Trigger Auto-Matching:**

```typescript
// For each of 212 NEW JOBS just scraped:
for (const job of newJobs) {
  
  // Calculate 6-factor match score
  const matchScore = calculateMatch(userResume, job);
  
  // Skill Match (40%)
  const userSkills = ["Java", "Spring", "MongoDB"];
  const jobSkills = ["Java", "Spring Boot", "MySQL"];
  skillScore = (2/3) * 40 = 26.67

  // Role Match (20%)
  const userRole = "Backend Developer";
  const jobRole = "Senior Backend Developer";
  roleScore = 15 // Partial match

  // Career Level Match (15%)
  // + Experience Match (10%)
  // + Location Match (10%)
  // + Work Mode Match (5%)
  
  totalScore = 26.67 + 15 + ... = 87 // EXCELLENT
  
  // Save to job_matches collection
  await JobMatch.create({
    userId: user._id,
    jobId: job._id,
    totalScore: 87,
    matchType: "excellent",
    skillScore: 26.67,
    roleScore: 15,
    ...
  });
  
  // Send notification
  if (totalScore >= 80) {
    await notificationService.send({
      userId: user._id,
      type: "excellent_match",
      job: job,
      score: 87
    });
  }
}
```

### Step 11: User Views Matched Jobs

```
URL: http://localhost:8080/dashboard/matches
```

**User sees:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Senior React Developer @ TechCorp [87/100] â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚
â”‚ Skill Match:        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 40/40
â”‚ Role Match:         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   15/20
â”‚ Level Match:        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 15/15
â”‚ Experience Match:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 10/10
â”‚ Location Match:     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 10/10
â”‚ Work Mode Match:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      5/5
â”‚
â”‚ Reason: High skill match + Perfect role fit
â”‚
â”‚ [View Job] [Save] [Apply]
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”— Complete Data Flow Diagram

```
ADMIN DASHBOARD
    â†“
[Admin clicks "Crawlers" sidebar]
    â†“
/admin/crawlers page loads
    â†“
GET /api/admin/scrape/logs
    â†“ (returns existing logs)
    â†“
[Admin selects buckets + clicks "Start Scraping"]
    â†“
POST /api/admin/scrape/run
{
  buckets: ["fresher", "software"],
  triggeredBy: "admin"
}
    â†“ (response: sessionId)
    â†“
[Backend processes in background]
    â”œâ”€ OpenWeb Ninja API (rate limited: 1 req/sec)
    â”œâ”€ Job Normalization (30+ fields)
    â”œâ”€ Deduplication (by externalJobId)
    â”œâ”€ MongoDB Save (jobs collection)
    â””â”€ Log Creation (scraping_logs collection)
    â†“
[Frontend auto-refreshes logs]
    â†“
GET /api/admin/scrape/logs
    â†“ (returns: new scraping log with status)
    â†“
[User uploads resume]
    â†“
POST /api/resume/upload
    â”œâ”€ Extract text (PDF/DOCX)
    â”œâ”€ Parse skills (regex)
    â””â”€ Save to parsed_resumes
    â†“
[Auto-trigger matching]
    â”œâ”€ Calculate 6-factor score for each job
    â”œâ”€ Create job_matches records
    â””â”€ Send notifications
    â†“
[User sees matched jobs]
    â†“
GET /api/matches/my-jobs
    â†“ (returns: jobs sorted by match score)
    â†“
[Display with score breakdown + apply button]
```

---

## ğŸ“Œ Key Takeaways

âœ… **Admin Workflow:**
1. Login â†’ Open Crawlers â†’ Select buckets â†’ Click "Start"
2. Backend scrapes OpenWeb Ninja API
3. Normalizes jobs (30+ fields)
4. Deduplicates by externalJobId
5. Saves 200+ new jobs to MongoDB
6. Creates audit log

âœ… **User Workflow:**
1. Upload resume
2. Auto-matching triggers
3. Calculates 6-factor scores
4. Creates match records
5. Shows matched jobs with score breakdown
6. User applies directly

âœ… **Database:**
- **jobs:** 1000s of jobs from scraping
- **parsed_resumes:** User resume data
- **job_matches:** Match scores for each user-job pair
- **scraping_logs:** Audit trail of all scraping sessions
- **api_usage:** Monthly API budget tracking

âœ… **API Limits:**
- 200 calls/month (OpenWeb Ninja)
- 1 request/second (rate limiting)
- Warning at 80% (160 calls)
- Hard stop at 200/month

---

## ğŸ¯ Current Status

âœ… Phase 1-5 Complete
âœ… Admin pages working (11 pages)
âœ… Crawlers page: Scraping UI ready
âœ… Backend: Endpoints configured
âœ… Ready for: Live scraping testing

**Next:** Test with real OpenWeb Ninja API key!
