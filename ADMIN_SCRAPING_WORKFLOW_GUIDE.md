# JobIntel Admin Scraping Workflow - Complete Guide

**Date:** January 19, 2026  
**Version:** 1.0  
**Status:** Based on Phase 2 Analysis

---

## ğŸ“‹ QUICK ANSWER

âœ… **YES** - The Crawlers page IS added to the admin sidebar  
âœ… **Path:** `/admin/crawlers`  
âœ… **Icon:** Globe icon  
âœ… **Position:** 8th item in admin menu

---

## ğŸ¯ HOW ADMIN SCRAPES DATA - COMPLETE WORKFLOW

### Step 1: Admin Navigates to Crawlers Page

**UI Flow:**
```
Admin Dashboard
    â†“
Left Sidebar (AdminSidebar.tsx)
    â†“
Click "Crawlers" menu item (Globe icon)
    â†“
Route: /admin/crawlers
    â†“
AdminCrawlers.tsx component loads
```

**Frontend Page:** `AdminCrawlers.tsx`
- Shows list of current scraping sources
- Form to add new sources/crawler configurations
- Status of crawlers
- Historical logs

---

### Step 2: Trigger Scraping from Admin Dashboard

**What Admin Sees:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Crawler Sources                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  [ ] Name     | URL     | Selector      â”‚
â”‚  [ ] LinkedIn | https:// | .jobcard    â”‚
â”‚  [ ] Indeed   | https:// | .job-list   â”‚
â”‚  [ ] Stack... | https:// | .job-item   â”‚
â”‚                                         â”‚
â”‚  [+ Add New Crawler]                    â”‚
â”‚  [âš™ Run Crawlers] â† MAIN ACTION        â”‚
â”‚  [ğŸ“Š Crawler Logs]                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Admin Actions:**
1. **Option A:** Click `[Run Crawlers]` button to start immediate scraping
2. **Option B:** Select specific buckets to scrape (fresher, software, data, etc.)
3. **Option C:** View scraping history/logs

---

### Step 3: API Endpoints Called (Backend)

When admin clicks "Run Crawlers", these endpoints are triggered:

#### **Endpoint 1: Start Scraping**
```
POST /api/admin/scrape/run

Request Body:
{
  "buckets": ["fresher", "software", "data", "cloud"],  // 11 possible buckets
  "triggeredBy": "admin"
}

Response:
{
  "sessionId": "abc-123-def-456",  // Unique session ID for tracking
  "message": "Scraping started",
  "status": "in-progress"
}
```

**PHASE 2 Controller Code (adminController.ts):**
```typescript
export async function runCrawlers(req: AuthRequest, res: Response) {
  const { buckets } = req.body;
  
  // Validate buckets
  const validBuckets = [
    'fresher', 'batch', 'software', 'data', 'cloud', 
    'mobile', 'qa', 'non-tech', 'experience', 'employment', 'work-mode'
  ];
  
  const invalidBuckets = buckets.filter(b => !validBuckets.includes(b));
  if (invalidBuckets.length > 0) {
    return res.status(400).json({ error: 'Invalid buckets' });
  }

  // Queue scraping job
  const sessionId = uuidv4();
  await scrapingQueue.add('scrape', { 
    buckets, 
    sessionId,
    triggeredByUserId: req.userId 
  });

  res.json({ sessionId, message: 'Scraping started', status: 'in-progress' });
}
```

---

### Step 4: Backend Processing (OpenWeb Ninja Integration)

**Flow Chart:**
```
Backend Receives Scraping Request
  â†“
[BullMQ Job Queue] - Queues the scraping task
  â†“
[Rate Limiter] - Checks API budget (200 calls/month)
  â†“
[OpenWeb Ninja Client] - Makes API calls (1 request/second)
  â†“
FOR EACH BUCKET (11 total):
  â”œâ”€ Query OpenWeb Ninja API with bucket keyword
  â”œâ”€ Receive raw job data
  â”œâ”€ Store in Redis queue temporarily
  â””â”€ Move to normalization
  â†“
[Job Normalization Service] - Extract 30+ fields
  â”œâ”€ Parse career level (fresher/junior/mid/senior)
  â”œâ”€ Detect domain (software/data/cloud/mobile/qa)
  â”œâ”€ Extract tech stack (React, Node.js, AWS, etc)
  â”œâ”€ Set expiry date (30 days from now)
  â””â”€ Calculate confidence score
  â†“
[Deduplication Service] - Check for duplicates
  â”œâ”€ Look up externalJobId in MongoDB
  â”œâ”€ If NEW: Insert new job document
  â”œâ”€ If DUPLICATE: Update existing job with latest info
  â””â”€ Prevent duplicate job listings
  â†“
[MongoDB] - Store normalized jobs
  Collection: "jobs"
  â”œâ”€ Fields: title, company, location, skills, careerLevel...
  â”œâ”€ Index by externalJobId (unique constraint)
  â””â”€ Mark isActive = true
  â†“
[API Usage Tracker] - Record this call
  â”œâ”€ Increment totalCallsUsed
  â”œâ”€ Check if over 200 limit (HARD STOP)
  â””â”€ Log bucket, keyword, result count
  â†“
[Scraping Log] - Record session details
  â”œâ”€ Start time, end time
  â”œâ”€ Jobs found, new jobs added, updated
  â”œâ”€ Success/failure status
  â””â”€ Any errors or rate limit issues
```

---

### Step 5: Admin Monitors Progress

#### **Endpoint 2: Check Scraping Status**
```
GET /api/admin/scrape/status/:sessionId

Response:
{
  "sessionId": "abc-123-def-456",
  "status": "in-progress",     // or "completed", "failed", "partial"
  "progress": 45,               // percentage 0-100
  "bucketsCompleted": ["fresher", "batch", "software"],
  "bucketsFailed": [],
  "totalJobsFound": 245,
  "newJobsAdded": 189,
  "jobsUpdated": 56,
  "estimatedTimeRemaining": "5 minutes"
}
```

**Frontend Updates:**
Admin sees a progress bar showing real-time status:
```
â”œâ”€ Fresher: âœ… Completed (34 jobs)
â”œâ”€ Batch: âœ… Completed (12 jobs)
â”œâ”€ Software: â³ In Progress... (45%)
â”œâ”€ Data: â± Queued...
â””â”€ Cloud: â± Queued...

Overall: 45% Complete
ETA: 5 minutes
Jobs Found: 245 | New: 189 | Updated: 56
```

---

### Step 6: Scraping Completes

#### **Endpoint 3: Get Scraping Logs**
```
GET /api/admin/scrape/logs

Response:
{
  "logs": [
    {
      "sessionId": "abc-123-def-456",
      "startedAt": "2025-01-19T10:30:00Z",
      "completedAt": "2025-01-19T10:35:45Z",
      "duration": "5 minutes 45 seconds",
      "status": "completed",
      "buckets": ["fresher", "batch", "software", "data", "cloud"],
      "totalApiCalls": 11,
      "totalJobsFound": 245,
      "newJobsAdded": 189,
      "jobsUpdated": 56,
      "apiCallsUsed": 11,
      "apiCallsRemaining": 189,  // Out of 200/month
      "triggeredBy": "admin"
    }
  ],
  "total": 1
}
```

---

### Step 7: Jobs Saved to MongoDB

**Collection: `jobs`**
```javascript
{
  _id: ObjectId("64f1a2b3c4d5e6f7g8h9i0j1"),
  
  // From API
  externalJobId: "openwebninja_12345",
  source: "OpenWeb Ninja JSearch API",
  bucket: "fresher",
  
  // Normalized Data
  title: "Junior Full Stack Developer",
  companyName: "TechCorp India",
  location: "Bangalore",
  
  // Extracted Fields
  careerLevel: "fresher",
  domain: "software",
  techStack: ["React", "Node.js", "MongoDB", "AWS"],
  experienceRequired: 0,
  workMode: "hybrid",
  
  // Metadata
  description: "...",
  requirements: ["React", "JavaScript", "Communication"],
  responsibilities: ["Develop features", "Debug issues", "Write tests"],
  applyUrl: "https://company.com/apply/123",
  
  // Lifecycle
  fetchedAt: 2025-01-19T10:35:00Z,
  expiryDate: 2025-02-18T10:35:00Z,  // 30 days
  isActive: true,
  parseQuality: "high",
  parseConfidence: 92,
  
  createdAt: 2025-01-19T10:35:00Z,
  updatedAt: 2025-01-19T10:35:00Z
}
```

---

### Step 8: Matching Begins (Automatic)

**Background Process (Phase 3):**
```
MongoDB receives new jobs
  â†“
For each new job, trigger batch matching:
  â”œâ”€ Query all users with uploaded resumes
  â”œâ”€ Calculate 6-factor match score for each user
  â”œâ”€ Store matches in job_matches collection
  â””â”€ Create notifications
  â†“
Create JobMatch Documents:
{
  userId: "user123",
  jobId: "job456",
  skillMatch: 40,        // 0-40 points
  roleMatch: 18,         // 0-20 points
  levelMatch: 15,        // 0-15 points
  experienceMatch: 8,    // 0-10 points
  locationMatch: 10,     // 0-10 points
  workModeMatch: 5,      // 0-5 points
  totalScore: 96,        // Total: 0-100
  matchType: "excellent" // â­â­â­
}
```

---

### Step 9: Users See Matched Jobs (Frontend)

**User Dashboard Flow:**
```
User logs in to /dashboard
  â†“
Clicks "My Matches" or "/matches"
  â†“
Frontend calls: GET /api/matching/my-jobs
  â†“
Shows matched jobs sorted by score:
  
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸŸ¢ Excellent Match (96/100)          â”‚
â”‚ Junior Full Stack Developer          â”‚
â”‚ TechCorp India | Bangalore | Hybrid  â”‚
â”‚                                      â”‚
â”‚ Skills: 40/40 âœ…                     â”‚
â”‚ Role: 18/20 âœ…                       â”‚
â”‚ Level: 15/15 âœ…                      â”‚
â”‚ Location: 10/10 âœ…                   â”‚
â”‚                                      â”‚
â”‚ [View Details] [Save] [Apply]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š 11 JOB BUCKETS EXPLAINED

When admin clicks "Run Crawlers", these 11 buckets are scraped:

| Bucket | Keywords | Example |
|--------|----------|---------|
| **1. Fresher** | entry, junior, graduate, trainee | Junior Developer, Fresher Engineer |
| **2. Batch** | batch, campus, intern, placement | Campus Hiring, Internship Program |
| **3. Software** | developer, engineer, python, java | Software Engineer, Full Stack Dev |
| **4. Data** | data, analytics, science, ml, ai | Data Scientist, ML Engineer |
| **5. Cloud** | cloud, devops, aws, azure, gcp | DevOps Engineer, Cloud Architect |
| **6. Mobile** | mobile, ios, android, react-native | iOS Developer, Mobile Engineer |
| **7. QA** | qa, test, automation, quality | QA Engineer, Test Automation |
| **8. Non-Tech** | sales, hr, marketing, support | Sales Executive, HR Manager |
| **9. Experience** | senior, lead, principal, expert | Senior Developer, Tech Lead |
| **10. Employment** | full-time, part-time, contract | Full-time, Contract Based |
| **11. Work-Mode** | remote, wfh, onsite, hybrid | Remote, WFH, Hybrid |

---

## ğŸ” ADMIN SIDEBAR - COMPLETE MENU

**File:** `AdminSidebar.tsx`

```
Navigation Items (11 total):
â”œâ”€ ğŸ“Š Dashboard (/admin)
â”œâ”€ ğŸ’¼ Jobs (/admin/jobs)
â”œâ”€ ğŸ‘¥ Users (/admin/users)
â”œâ”€ ğŸ“„ Profile Fields (/admin/profile-fields)
â”œâ”€ ğŸ† Skills (/admin/skills)
â”œâ”€ ğŸ”” Notifications (/admin/notifications)
â”œâ”€ ğŸ¤ Referrals (/admin/referrals)
â”œâ”€ ğŸŒ Crawlers (/admin/crawlers)         â† SCRAPING PAGE
â”œâ”€ ğŸ“ˆ Analytics (/admin/analytics)
â”œâ”€ ğŸ’³ Revenue (/admin/revenue)
â”œâ”€ âš™ï¸ Settings (/admin/settings)
â””â”€ Exit Admin â† Logout button
```

---

## ğŸš€ API RATE LIMITING - CRITICAL CONSTRAINT

**Monthly Budget:** 200 API calls/month (OpenWeb Ninja Free Tier)

**How It Works:**

```
API Usage Collection: api_usage
{
  month: "2025-01",
  totalCallsUsed: 45,      // Current month usage
  monthlyLimit: 200,       // Admin-configurable
  callsRemaining: 155,     // 200 - 45
  
  isWarningTriggered: false,    // True at 80% (160 calls)
  isLimitReached: false,        // True at 200% (HARD STOP)
  
  callHistory: [
    {
      timestamp: "2025-01-19T10:35:00Z",
      bucket: "fresher",
      resultCount: 23,
      status: "success",
      initiatedBy: "admin_user_id"
    },
    // ... more calls
  ]
}
```

**Admin Checks Usage:**
```
GET /api/admin/api-usage

Response:
{
  "totalCallsUsed": 45,
  "monthlyLimit": 200,
  "callsRemaining": 155,
  "percentageUsed": 22.5,
  "isWarningTriggered": false,      // Warning at 80%
  "isLimitReached": false,          // Hard stop at 100%
  "safetyThreshold": 160            // 80% of 200
}
```

**Frontend Display:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Usage This Month        â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚ [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 45/200  â”‚
â”‚ 22.5% used | 155 remaining  â”‚
â”‚                             â”‚
â”‚ Status: âœ… Good to scrape   â”‚
â”‚ Next Reset: Feb 01, 2025    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“± ADMIN CRAWLERS PAGE - UI COMPONENTS

**File:** `AdminCrawlers.tsx`

**Page Structure:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Admin Dashboard > Crawlers                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                  â”‚
â”‚  [1] ğŸ“Š API Usage Status                        â”‚
â”‚      â”œâ”€ Total Calls Used: 45/200               â”‚
â”‚      â”œâ”€ Calls Remaining: 155                    â”‚
â”‚      â””â”€ Status: âœ… Good                         â”‚
â”‚                                                  â”‚
â”‚  [2] âš™ï¸ Quick Actions                          â”‚
â”‚      â”œâ”€ [Run All Crawlers]   â† MAIN BUTTON     â”‚
â”‚      â”œâ”€ [Select Buckets]     â† Custom selection â”‚
â”‚      â”œâ”€ [Cancel Active Job]  â† If running      â”‚
â”‚      â””â”€ [View Logs]          â† History         â”‚
â”‚                                                  â”‚
â”‚  [3] ğŸ” Crawler Sources                        â”‚
â”‚      â”œâ”€ LinkedIn Sources                        â”‚
â”‚      â”œâ”€ Indeed Sources                          â”‚
â”‚      â”œâ”€ Stack Overflow Sources                  â”‚
â”‚      â””â”€ [+ Add New Source]   â† Custom sources  â”‚
â”‚                                                  â”‚
â”‚  [4] ğŸ“‹ Scraping History                       â”‚
â”‚      â”œâ”€ Session ID | Date | Status | Jobs Foundâ”‚
â”‚      â”œâ”€ abc-123 | Jan 19 | âœ… Complete | 245  â”‚
â”‚      â”œâ”€ def-456 | Jan 18 | âœ… Complete | 189  â”‚
â”‚      â””â”€ ghi-789 | Jan 17 | âœ… Complete | 212  â”‚
â”‚                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ COMPLETE USER JOURNEY AFTER SCRAPING

```
1ï¸âƒ£ ADMIN SCRAPES DATA
   â””â”€ Clicks "Run Crawlers" on /admin/crawlers

2ï¸âƒ£ DATA STORED TO MONGODB
   â””â”€ Jobs collection has 10,000+ new jobs

3ï¸âƒ£ MATCHING TRIGGERED (Automatic)
   â””â”€ For each user with resume:
      â”œâ”€ Calculate 6-factor match for each job
      â”œâ”€ Filter jobs with score >= 50%
      â”œâ”€ Create JobMatch documents
      â””â”€ Queue notifications (email, WhatsApp, Telegram)

4ï¸âƒ£ USER SEES NOTIFICATIONS
   â””â”€ Email: "ğŸ”¥ You have 12 new job matches!"
   â””â”€ WhatsApp: "Hey! Check out 12 jobs tailored for you"

5ï¸âƒ£ USER VIEWS MATCHES
   â””â”€ Goes to /matches
   â””â”€ Sees jobs sorted by match score
   â””â”€ Filters by: excellent (80+), good (60-79), okay (50-59)

6ï¸âƒ£ USER APPLIES FOR JOB
   â””â”€ Clicks [Apply]
   â””â”€ Redirected to job's apply URL (LinkedIn, Indeed, etc)
   â””â”€ Job marked as "applied" in SavedJobs

7ï¸âƒ£ ADMIN SEES ANALYTICS
   â””â”€ Views /admin/analytics
   â””â”€ Tracks: Matches created, Notifications sent, Applications
```

---

## âœ… ADMIN SCRAPING - QUICK REFERENCE

| Question | Answer |
|----------|--------|
| **Page Location?** | `/admin/crawlers` |
| **Sidebar Item?** | YES - "Crawlers" with Globe icon (8th position) |
| **How to access?** | Login as admin â†’ Click "Crawlers" in sidebar |
| **What does it do?** | Triggers scraping of 11 job buckets from OpenWeb Ninja |
| **Rate limit?** | 200 calls/month (OpenWeb Ninja free tier) |
| **Who scrapes?** | Admin triggers manually (or cron job automatically) |
| **Data stored?** | MongoDB `jobs` collection |
| **Users see matches?** | YES - Automatically matched and notified |
| **Real-time progress?** | YES - Progress bar shows status |
| **Can cancel?** | YES - `POST /api/admin/scrape/cancel/:sessionId` |
| **View history?** | YES - `GET /api/admin/scrape/logs` |

---

## ğŸ“ PHASE REFERENCE

| Phase | What Happens |
|-------|--------------|
| **Phase 1** | Infrastructure setup, database schema, API client |
| **Phase 2** | Admin scraping endpoints + UI page created âœ… |
| **Phase 3** | Job normalization, matching algorithm (you're here) |
| **Phase 4** | Resume parsing, auto-matching (you're here) |
| **Phase 5** | Notifications (email, WhatsApp, Telegram) |

---

**Summary:** Admin scrapes by clicking one button â†’ Jobs saved to MongoDB â†’ Users get automatic match notifications â†’ Users see matched jobs on dashboard â†’ Users apply â†’ Admin sees analytics. All controlled from the admin dashboard "Crawlers" page! ğŸš€

